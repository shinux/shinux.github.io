<!DOCTYPE html><html><head><meta charset="UTF-8"><title>elasticsearch migrate data.</title><link rel="stylesheet" href="css/normalize.css"><link rel="stylesheet" href="/css/hexo-theme-adoubi.css"><link rel="icon" href="/images/favicon.ico"></head><body><div class="header"><a class="github-link" href="https://github.com/shinux"><img class="github-image" src="/images/Aquicon-Github-small.png"></a><a class="email-link" href="mailto:askb@me.com"><img class="email-image" src="/images/email-small.png"></a><a class="subscribe-link" href="/atom.xml"><img class="subscribe-image" src="/images/subscribe-small.png"></a></div><div class="content"><div class="post-item"></div><h2 class="post-title-wrapper"><p class="post-title">elasticsearch migrate data.</p></h2><div class="post-date"><time datetime="2018-07-17T15:46:16.000Z">2018-07-17</time></div><div class="post-content"><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ul>
<li>数据增长过快，原有服务器磁盘不足，而云服务器磁盘不支持不停机升级</li>
<li>依赖 es 的增删改查的 HTTP 请求频繁，希望 migration 过程能做到 zero downtime</li>
<li>single node 导致的没有 replica shards，服务不够稳定，可用性差</li>
<li>希望形成真正的 cluster</li>
<li>考虑之后的扩展能力</li>
</ul>
<h3 id="Migration"><a href="#Migration" class="headerlink" title="Migration"></a>Migration</h3><p>先说结论，最终是没有实现 zero downtime。参考了几个不停机迁移方案以及现有的业务模型和数据规模，感觉过程实现成本和风险远远大于短暂的停机，所以选择了在凌晨重启服务加入了两个新的节点构成集群并在 data nodes 之间同步数据，修改配置和加入集群期间服务不可用大概一分钟多，新的 data node 同步百G级数据完成大概用了4个小时。这一分钟左右没有出现更新操作（凌晨原因），新增操作在后续通过脚本做了补充。</p>
<p>关于不停机迁移的方法也看了几个现成的方法诸如</p>
<p><a href="https://blog.engineering.ticketbis.com/elasticsearch-cluster-migration-without-downtime/" target="_blank" rel="external">https://blog.engineering.ticketbis.com/elasticsearch-cluster-migration-without-downtime/</a><br><a href="https://thoughts.t37.net/migrating-a-130tb-cluster-from-elasticsearch-2-to-5-in-20-hours-with-0-downtime-and-a-rollback-39b4b4f29119" target="_blank" rel="external">https://thoughts.t37.net/migrating-a-130tb-cluster-from-elasticsearch-2-to-5-in-20-hours-with-0-downtime-and-a-rollback-39b4b4f29119</a></p>
<p>多半离不开版本号和更新时间这两个字段，迁移过程区分迁移前的数据和迁移过程中新增的数据，在迁移完毕时把新数据再同步一次，这就要求数据只有 create 没有 update 操作在迁移过程中发生，感觉不太适合除了自增类型的例如日志以外的其他业务逻辑。</p>
<p>关于集群，需要至少3个 master eligible node 以防止脑裂，并把其中两个设置为 data node 承载数据，在磁盘的扩充过程中给每个节点挂载数据盘，方便日后扩容。</p>
<p><img src="http://obfnm2kw5.bkt.clouddn.com/clients.png"></p>
<p>以其中一个节点为例，几个比较重要的配置是</p>
<pre><code>discovery.zen.ping.unicast.hosts: [&quot;10.66.90.77:9300&quot;, &quot;10.66.91.150:9300&quot;, &quot;10.30.55.37:9300&quot;]
network.host: [&quot;10.30.55.37&quot;, _local_]
</code></pre><p>作为有限且小规模的集群，为 discovery module 指定广播发现节点的私有地址列表，同时允许各个节点在本机的访问调试.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>总之如果不是追求绝对的不停机，并且不愿意增加额外的字段或改变自己的逻辑，es提供的加入集群方式来做迁移数据在我看来是相当简便又稳定的一种方式。</p>
<p>后续应该会在众多开源产品中选择一种 monitor 让集群和服务的状态更透明、报警和响应更及时。</p>
</div></div><div class="footer"><div class="footer-copyright">Theme By <a href="https://github.com/shinux/hexo-theme-adoubi">Adoubi</a> , Powered By Hexo.</div></div></body></html>