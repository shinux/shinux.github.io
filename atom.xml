<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sinux</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-09-16T16:31:40.996Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Sinux</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于 es 的 query cache</title>
    <link href="http://yoursite.com/2020/08/13/es-cache/"/>
    <id>http://yoursite.com/2020/08/13/es-cache/</id>
    <published>2020-08-13T07:46:16.000Z</published>
    <updated>2020-09-16T16:31:40.996Z</updated>
    
    <content type="html"><![CDATA[<h3 id="查看缓存占用的内存数量"><a href="#查看缓存占用的内存数量" class="headerlink" title="查看缓存占用的内存数量"></a>查看缓存占用的内存数量</h3><p>基于一个 8 节点集群，查询 QPS 大概 30 左右。通过 node 信息查看请求缓存</p><p><code>curl localhost:9200/_nodes/stats/indices/request_cache?pretty</code></p><p>执行结果:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;_nodes&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 8,</span><br><span class="line">    &quot;successful&quot; : 8,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;moka-es&quot;,</span><br><span class="line">  &quot;nodes&quot; : &#123;</span><br><span class="line">    &quot;4FbqP65LTf-uwF1sGZAnQQ&quot; : &#123;</span><br><span class="line">      &quot;timestamp&quot; : 1597291580728,</span><br><span class="line">      &quot;name&quot; : &quot;es-node-3&quot;,</span><br><span class="line">      &quot;transport_address&quot; : &quot;172.17.240.18:9300&quot;,</span><br><span class="line">      &quot;host&quot; : &quot;172.17.240.18&quot;,</span><br><span class="line">      &quot;ip&quot; : &quot;172.17.240.18:9300&quot;,</span><br><span class="line">      &quot;roles&quot; : [</span><br><span class="line">        &quot;master&quot;,</span><br><span class="line">        &quot;data&quot;,</span><br><span class="line">        &quot;ingest&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;attributes&quot; : &#123;</span><br><span class="line">        &quot;ml.machine_memory&quot; : &quot;33738051584&quot;,</span><br><span class="line">        &quot;ml.max_open_jobs&quot; : &quot;20&quot;,</span><br><span class="line">        &quot;xpack.installed&quot; : &quot;true&quot;,</span><br><span class="line">        &quot;ml.enabled&quot; : &quot;true&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;indices&quot; : &#123;</span><br><span class="line">        &quot;request_cache&quot; : &#123;</span><br><span class="line">          &quot;memory_size_in_bytes&quot; : 53328,</span><br><span class="line">          &quot;evictions&quot; : 0,</span><br><span class="line">          &quot;hit_count&quot; : 1395,</span><br><span class="line">          &quot;miss_count&quot; : 10267</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;q0_ZkHoASzqRsGrCVUlQPA&quot; : &#123;</span><br><span class="line">      &quot;timestamp&quot; : 1597291580729,</span><br><span class="line">      &quot;name&quot; : &quot;es-node-8&quot;,</span><br><span class="line">      &quot;transport_address&quot; : &quot;172.17.47.73:9300&quot;,</span><br><span class="line">      &quot;host&quot; : &quot;172.17.47.73&quot;,</span><br><span class="line">      &quot;ip&quot; : &quot;172.17.47.73:9300&quot;,</span><br><span class="line">      &quot;roles&quot; : [</span><br><span class="line">        &quot;master&quot;,</span><br><span class="line">        &quot;data&quot;,</span><br><span class="line">        &quot;ingest&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;attributes&quot; : &#123;</span><br><span class="line">        &quot;ml.machine_memory&quot; : &quot;32300605440&quot;,</span><br><span class="line">        &quot;ml.max_open_jobs&quot; : &quot;20&quot;,</span><br><span class="line">        &quot;xpack.installed&quot; : &quot;true&quot;,</span><br><span class="line">        &quot;ml.enabled&quot; : &quot;true&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;indices&quot; : &#123;</span><br><span class="line">        &quot;request_cache&quot; : &#123;</span><br><span class="line">          &quot;memory_size_in_bytes&quot; : 16287,</span><br><span class="line">          &quot;evictions&quot; : 0,</span><br><span class="line">          &quot;hit_count&quot; : 23,</span><br><span class="line">          &quot;miss_count&quot; : 74</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">....</span><br></pre></td></tr></table></figure><p>结果比较奇怪，相较于每个节点 32GB 的内存，几百 kb 的换存量几乎相当于没有缓存任何请求结果。</p><p>而随便找一台测试用的单机 es ，缓存量反而比上面集群的缓存总和还要多:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;_nodes&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 1,</span><br><span class="line">    &quot;successful&quot; : 1,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,</span><br><span class="line">  &quot;nodes&quot; : &#123;</span><br><span class="line">    &quot;ccC4H-1mR4apSB5pgR6yXw&quot; : &#123;</span><br><span class="line">      &quot;timestamp&quot; : 1597292348183,</span><br><span class="line">      &quot;name&quot; : &quot;ccC4H-1&quot;,</span><br><span class="line">      &quot;transport_address&quot; : &quot;172.17.46.126:9300&quot;,</span><br><span class="line">      &quot;host&quot; : &quot;172.17.46.126&quot;,</span><br><span class="line">      &quot;ip&quot; : &quot;172.17.46.126:9300&quot;,</span><br><span class="line">      &quot;roles&quot; : [</span><br><span class="line">        &quot;master&quot;,</span><br><span class="line">        &quot;data&quot;,</span><br><span class="line">        &quot;ingest&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;attributes&quot; : &#123;</span><br><span class="line">        &quot;ml.machine_memory&quot; : &quot;33191542784&quot;,</span><br><span class="line">        &quot;xpack.installed&quot; : &quot;true&quot;,</span><br><span class="line">        &quot;ml.max_open_jobs&quot; : &quot;20&quot;,</span><br><span class="line">        &quot;ml.enabled&quot; : &quot;true&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;indices&quot; : &#123;</span><br><span class="line">        &quot;request_cache&quot; : &#123;</span><br><span class="line">          &quot;memory_size_in_bytes&quot; : 2653163,</span><br><span class="line">          &quot;evictions&quot; : 0,</span><br><span class="line">          &quot;hit_count&quot; : 1353,</span><br><span class="line">          &quot;miss_count&quot; : 2439</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="什么样的-query-会被-cache"><a href="#什么样的-query-会被-cache" class="headerlink" title="什么样的 query 会被 cache ?"></a>什么样的 query 会被 cache ?</h3><blockquote><p>It is not possible to look at the contents being cached.</p></blockquote><p>而且除非自己 log，es 也没办反看到 query history（？）我们只能通过 node API，index 级别的配置和 query type 去判断缓存的使用情况。</p><p>这里不同 doc 的文档、书籍、网上的文章说的有点出入。在我的理解里，只有两种 query cache， node query cache 和 shards query cache。</p><h4 id="node-query-cache"><a href="#node-query-cache" class="headerlink" title="node query cache"></a>node query cache</h4><p>node query cache 只保留 filter 类型的 query result。</p><p>es 的 node query cache 默认是开启的，基于 LRU（least recently use），当缓存达到上限，一条新缓存出现的同时会驱逐（evict）最近最少使用的那一条。</p><p>从 query 类型来看，除了 filter 类型的 query ，其他任何类型的 query 结果都不会被缓存。从数量和大小来看，默认会保存最多一万条或最多 10% 的 heap size 的 query。因 cache 基于 segment, 当 segment（一个真实的倒排索引文件）的 doc 数量小于一万条或小于当前 shards 的 3% 或者总的请求数量过小时，都不会触发 cache。</p><h4 id="shard-request-cache"><a href="#shard-request-cache" class="headerlink" title="shard request cache"></a>shard request cache</h4><p>shard request cache 就是把在各个 shard 上执行的 query 本地结果缓存下来。 一样也是 LRU 机制。他会缓存频繁请求的结果或者通过对每一个 query 手动添加 <code>request_cache=true</code> 来决定一个请求是否触发缓存。</p><p>注意，这里默认只会缓存 size = 0 的 query 结果，也就是一些可以不在意 hits 的 query，比如 aggregation，suggestion。这意味着如果不手动标识，日常针对普通 field 的搜索想要召回具体命中 doc 的 query 全部不会被缓存。这也解释了为什么上述的集群几乎没有 request cache。</p><h3 id="其他方案"><a href="#其他方案" class="headerlink" title="其他方案"></a>其他方案</h3><p>其他的缓存方案大同小异，就是在得到结果后，配合 query 做一个 k-v 形式的缓存，区别只是缓存选型、数据更新、缓存过期时间上的处理。例如这个 <a href="https://medium.com/trackit/how-to-add-a-redis-caching-layer-to-your-elasticsearch-queries-ca56e5e84d9b" target="_blank" rel="noopener">利用 Redis 做缓存</a></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://doc.yonyoucloud.com/doc/mastering-elasticsearch/chapter-5/54_README.html" target="_blank" rel="noopener">Mastering Elasticsearch</a><br><a href="https://sematext.com/blog/elasticsearch-cache-usage/" target="_blank" rel="noopener">ElasticSearch Cache Usage</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-cache.html" target="_blank" rel="noopener">Query Cache</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;查看缓存占用的内存数量&quot;&gt;&lt;a href=&quot;#查看缓存占用的内存数量&quot; class=&quot;headerlink&quot; title=&quot;查看缓存占用的内存数量&quot;&gt;&lt;/a&gt;查看缓存占用的内存数量&lt;/h3&gt;&lt;p&gt;基于一个 8 节点集群，查询 QPS 大概 30 左右。通过 nod
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch with BM25</title>
    <link href="http://yoursite.com/2020/04/01/BM25/"/>
    <id>http://yoursite.com/2020/04/01/BM25/</id>
    <published>2020-04-01T07:18:16.000Z</published>
    <updated>2020-04-01T07:44:02.103Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ES-分数不唯一的问题"><a href="#ES-分数不唯一的问题" class="headerlink" title="ES 分数不唯一的问题"></a>ES 分数不唯一的问题</h3><p>每个 index 通常会分布在多个 shards ，例如默认配置是 5，相同原文在同一 query 下分数有偏差的主要原因是不同 shards 中 term 出现频率不一致（TF ）。</p><p>几种缓解的办法：</p><ol><li>足够多的 doc 可以拉平不同 shards 的差异</li><li>减少所用 index 的 shards 数量，和上面方法类似</li><li><code>?search_type=dfs_query_then_fetch</code>  （DFS Distributed Frequency Search） 集合所有 shards 的 TF 最后计算分数，等同于 number_of_shards = 1</li></ol><h3 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h3><p>BM25 score 的基本方程：</p><p><img src="/images/bm25_equation.png"></p><p><strong>IDF</strong>: 罕见的词区分度更高，分数加成更高<br><strong>fieldLen/avgFieldLen</strong>: 文本越长，总分数越低<br><strong>b</strong>: es 中默认是 0.75，越小，对于上述的当前文本长度与平均文本长度的比值对分数的影响越小，b 为 0 时，分数只与 count 和 relevance 呈正相关<br><strong>f(qi,D)</strong>: tf，文档D 中 query i 出现的频率，频率越高，分数越高<br><strong>k1</strong>: es 中默认是 1.2 ，控制 tf 的斜率，随着 term 出现次数逐渐降低对分数提升的影响，即<strong>Term frequency saturation（词频饱和度)</strong>，当 k1 是 0 时，等于只计算 IDF。</p><p>下图是传统 TF 与 BM25 TF 的分数变化曲线对比：</p><p><img src="/images/TF_comparison.png"></p><p>传统 tf/idf 的 tf 会无限提升分数，BM25 则在 tf 达到饱和后一定程度缩小了对相关性的影响。</p><h3 id="变量的使用"><a href="#变量的使用" class="headerlink" title="变量的使用"></a>变量的使用</h3><p>原则上不建议调整 b 和 k1 的默认值，优先考虑下列方法:</p><ol><li>最常用的 boost score on query field</li><li>近义词匹配</li><li>在 index 和 query 阶段分别通过 analyzer 提前处理</li><li>考虑几种 function score <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#score-functions" target="_blank" rel="noopener">Function score query | Elasticsearch Reference 7.6 | Elastic</a></li></ol><p>与小说、长文献不同，考虑 resume content  的场景，对于频繁出现技能和结果的词汇，可以适当降低 k 值，因为简历文档足够精简，信息确切，出现的 term 基本代表着一个人的信息，这时我们希望 term 迅速饱和；于此同时针对稍长的混杂着除职责本身的公司福利、发展状况、服务客户等 职位JD 场景，我们可以适当提高 b 值，把冗长的与用户搜索无关信息过滤掉。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf" target="_blank" rel="noopener">BM25 paper</a><br><a href="https://km.aifb.kit.edu/ws/semsearch10/Files/bm25f.pdf" target="_blank" rel="noopener">BM25F paper</a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html" target="_blank" rel="noopener">Index-modules-similarity</a><br><a href="https://www.elastic.co/cn/elasticon/conf/2016/sf/improved-text-scoring-with-bm25" target="_blank" rel="noopener">Improved Text Scoring with BM25 | Elastic</a><br><a href="https://en.wikipedia.org/wiki/Okapi_BM25" target="_blank" rel="noopener">Okapi BM25 - Wikipedia</a><br><a href="https://www.elastic.co/cn/blog/practical-bm25-part-1-how-shards-affect-relevance-scoring-in-elasticsearch" target="_blank" rel="noopener">Practical BM25 - Part 1: How Shards Affect Relevance Scoring in Elasticsearch | Elastic Blog</a><br><a href="https://www.elastic.co/cn/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables" target="_blank" rel="noopener">Practical BM25 - Part 2: The BM25 Algorithm and its Variables | Elastic Blog</a><br><a href="https://www.elastic.co/cn/blog/practical-bm25-part-3-considerations-for-picking-b-and-k1-in-elasticsearch" target="_blank" rel="noopener">Practical BM25 - Part 3: Considerations for Picking b and k1 in Elasticsearch | Elastic Blog</a><br><a href="https://www.elastic.co/cn/elasticon/conf/2016/sf/quantitative-cluster-sizing" target="_blank" rel="noopener">Quantitative Cluster Sizing | Elastic</a><br><a href="https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/" target="_blank" rel="noopener">BM25 The Next Generation of Lucene Relevance</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ES-分数不唯一的问题&quot;&gt;&lt;a href=&quot;#ES-分数不唯一的问题&quot; class=&quot;headerlink&quot; title=&quot;ES 分数不唯一的问题&quot;&gt;&lt;/a&gt;ES 分数不唯一的问题&lt;/h3&gt;&lt;p&gt;每个 index 通常会分布在多个 shards ，例如默认配置是 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>OQL &amp; JavaCC —— keynote of presentation</title>
    <link href="http://yoursite.com/2019/11/08/OQL_JavaCC/"/>
    <id>http://yoursite.com/2019/11/08/OQL_JavaCC/</id>
    <published>2019-11-08T07:46:16.000Z</published>
    <updated>2019-12-26T03:48:14.744Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/OQL_JavaCC.png"></p><p>Can’t upload demo due to demo code is already running on the company that I work for.</p><p>Though JavaCC is a compiler compiler or parser generator, I regard it as a compiler frontend which IR is merely Java.</p><p>For the next few weeks, I’m going to build an compiled language with JavaCC.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/OQL_JavaCC.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Can’t upload demo due to demo code is already running on the company that I work for.&lt;/p&gt;
&lt;p&gt;Th
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch</title>
    <link href="http://yoursite.com/2016/11/16/elasticsearch/"/>
    <id>http://yoursite.com/2016/11/16/elasticsearch/</id>
    <published>2016-11-16T07:46:16.000Z</published>
    <updated>2019-11-08T08:15:01.332Z</updated>
    
    <content type="html"><![CDATA[<h2 id="or-operation"><a href="#or-operation" class="headerlink" title="or operation"></a>or operation</h2><p>以下内容基于 Elasticsearch 2.0.1 。</p><p>首先是 <code>or filter</code> 已经变成了 <code>bool</code> ，它是专门用来合并 queries ，支持 or、and、not 这类操作符的，大概相当于 <code>()</code>(?).</p><ul><li><code>bool</code> 中的 <code>must</code> 就等于 SQL 中的 <code>AND</code></li><li>而 <code>should</code> 就相当于 SQL 中的 <code>OR</code></li><li><code>terms</code> 用来 match 任一出现在 simple type array (not object) 中的 doc</li></ul><p>当然还涉及到搜索结果评分等，但是这里不提这些。</p><p>现在遇到的情景大概就是想搜索多个空格分隔的关键字，任何一个 field 满足任一个关键字则匹配成功，返回所有匹配成功的结果和它们的 highlight 字段。</p><p>首先是对搜索内容做一点微不足道的处理:</p><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> queries = <span class="string">'some keywords'</span>.trim().split(<span class="regexp">/\s+/</span>).map(<span class="function">(<span class="params">query</span>) =&gt;</span> query.lowerCase());</span><br><span class="line"><span class="keyword">const</span> queryString = queries.join(<span class="string">' OR '</span>);</span><br></pre></td></tr></table></figure><p>保持所有 query 包裹在 bool 中，把上面的两个变量用在 POST body 中，curl 同理:</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"index"</span>: <span class="string">"index"</span>,</span><br><span class="line">    <span class="attr">"from"</span>: <span class="number">11</span>,</span><br><span class="line">    <span class="attr">"size"</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="attr">"_source"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"body"</span>: &#123;</span><br><span class="line">        <span class="attr">"query"</span>: &#123;</span><br><span class="line">            <span class="attr">"bool"</span>: &#123;</span><br><span class="line">            <span class="attr">"should"</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">"terms"</span>: &#123;</span><br><span class="line">                        <span class="attr">"tags"</span>: queries,</span><br><span class="line">                    &#125;,</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">"query_string"</span>: &#123;</span><br><span class="line">                        <span class="attr">"query"</span>: queryString,</span><br><span class="line">                        <span class="attr">"fields"</span>: [<span class="string">"name^5"</span>, <span class="string">"name.analyzed^5"</span>, <span class="string">"email"</span>, <span class="string">"phone"</span>, <span class="string">"content"</span>],</span><br><span class="line">                        <span class="attr">"analyzer"</span>: <span class="string">"whitespace"</span>,</span><br><span class="line">                    &#125;,</span><br><span class="line">                &#125;,</span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"minimum_should_match"</span>: <span class="number">1</span>,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"highlight"</span>: &#123;</span><br><span class="line">            <span class="attr">"fields"</span>: &#123;</span><br><span class="line">                "content": &#123; fragment_size: 18, number_of_fragments: 1 &#125;,</span><br><span class="line">                <span class="string">"name: &#123;&#125;,</span></span><br><span class="line">                "name.analyzed": &#123;&#125;,</span><br><span class="line">                "tags": &#123;&#125;,</span><br><span class="line">                "email": &#123;&#125;,</span><br><span class="line">                "phone": &#123;&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>note: <code>minimum_should_match</code> may not be available in some version of elasticsearch.</p><h2 id="es-版本升级"><a href="#es-版本升级" class="headerlink" title="es 版本升级"></a>es 版本升级</h2><h4 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h4><ul><li>本地下载 elasticsearch 新版本，传到服务器上解压，同时 chown -R 整个文件夹拥有者为非 root</li><li>所有 plugins checkout 到新版本，重新编译打包，例如 ik，然后 scp 上传到新版 elasticsearch 的 plugins 中并解压</li></ul><h4 id="Migration"><a href="#Migration" class="headerlink" title="Migration"></a>Migration</h4><ul><li>这两步<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html" target="_blank" rel="noopener">严格按照文档</a>，disable shard allocation 和 synced flush，方便到新的集群中快速 recovery</li><li>停掉旧版本的 cluster</li><li>完整 copy data 文件夹到新版本 elasticsearch 文件夹中</li><li>config/elasticsearch.yml 的内容需要针对新版本做出调整</li></ul><h4 id="Run"><a href="#Run" class="headerlink" title="Run"></a>Run</h4><ul><li>开启新版 elasticsearch ，同时 enable shard allocation</li><li>[optional] open all indices: <code>curl -XPOST &quot;http://localhost:9200/_all/_open&quot;</code></li><li>等待 shard recovery，在 synced flush 的帮助下这步不会太久</li></ul><p>从 2.0 -&gt; 5.2 整体来说没有想象中那么困难，当然原本的环境就是单点大法…所以过程比较顺畅，有一些小坑，错误都比较明显。</p><p>Referrer: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/setup-upgrade.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.3/setup-upgrade.html</a></p><h2 id="es-从单机到集群"><a href="#es-从单机到集群" class="headerlink" title="es 从单机到集群"></a>es 从单机到集群</h2><h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><ul><li>数据增长过快，原有服务器磁盘不足，而云服务器磁盘不支持不停机升级</li><li>依赖 es 的增删改查的 HTTP 请求频繁，希望 migration 过程能做到 zero downtime</li><li>single node 导致的没有 replica shards，服务不够稳定，可用性差</li><li>希望形成真正的 cluster</li><li>考虑之后的扩展能力</li></ul><h4 id="Migration-1"><a href="#Migration-1" class="headerlink" title="Migration"></a>Migration</h4><p>先说结论，最终是没有实现 zero downtime。参考了几个不停机迁移方案以及现有的业务模型和数据规模，感觉过程实现成本和风险远远大于短暂的停机，所以选择了在凌晨重启服务加入了两个新的节点构成集群并在 data nodes 之间同步数据，修改配置和加入集群期间服务不可用大概一分钟多，新的 data node 同步百G级数据完成大概用了4个小时。这一分钟左右没有出现更新操作（凌晨原因），新增操作在后续通过脚本做了补充。</p><p>关于不停机迁移的方法也看了几个现成的方法诸如</p><p><a href="https://blog.engineering.ticketbis.com/elasticsearch-cluster-migration-without-downtime/" target="_blank" rel="noopener">https://blog.engineering.ticketbis.com/elasticsearch-cluster-migration-without-downtime/</a><br><a href="https://thoughts.t37.net/migrating-a-130tb-cluster-from-elasticsearch-2-to-5-in-20-hours-with-0-downtime-and-a-rollback-39b4b4f29119" target="_blank" rel="noopener">https://thoughts.t37.net/migrating-a-130tb-cluster-from-elasticsearch-2-to-5-in-20-hours-with-0-downtime-and-a-rollback-39b4b4f29119</a></p><p>多半离不开版本号和更新时间这两个字段，迁移过程区分迁移前的数据和迁移过程中新增的数据，在迁移完毕时把新数据再同步一次，这就要求数据只有 create 没有 update 操作在迁移过程中发生，感觉不太适合除了自增类型的例如日志以外的其他业务逻辑。</p><p>关于集群，需要至少3个 master eligible node 以防止脑裂，并把其中两个设置为 data node 承载数据，在磁盘的扩充过程中给每个节点挂载数据盘，方便日后扩容。</p><p><img src="/images/cluster.png"></p><p>以其中一个节点为例，几个比较重要的配置是</p><pre><code>discovery.zen.ping.unicast.hosts: [&quot;10.66.90.77:9300&quot;, &quot;10.66.91.150:9300&quot;, &quot;10.30.55.37:9300&quot;]network.host: [&quot;10.30.55.37&quot;, _local_]</code></pre><p>作为有限且小规模的集群，为 discovery module 指定广播发现节点的私有地址列表，同时允许各个节点在本机的访问调试.</p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>总之如果不是追求绝对的不停机，并且不愿意增加额外的字段或改变自己的逻辑，es提供的加入集群方式来做迁移数据在我看来是相当简便又稳定的一种方式。</p><p>后续应该会在众多开源产品中选择一种 monitor 让集群和服务的状态更透明、报警和响应更及时。</p><h4 id="script-language"><a href="#script-language" class="headerlink" title="script language"></a>script language</h4><p>从 2.0.1 升级到 5.1.1 有一阵子了，今天发现了一个存在比较久的问题，就是诸如 update 包括 bulk update 操作，不能被正常的执行。问题集中在那些在 body 中使用 script 的 query，而直接全文更新的则没有问题。例如，对 list 类型的 document 进行局部更新:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST index/type/id/_update</span><br><span class="line">&#123;</span><br><span class="line">    &quot;script&quot;: &quot;ctx._source.tags -= tag&quot;,</span><br><span class="line">    &quot;params&quot; : &#123;</span><br><span class="line">        &quot;tag&quot; : &quot;blue&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样的使用在 2.0.1 中是没问题的，然而在 5.x 中却报错：<code>Variable [tag] is not defined.</code> 无法执行这个 script，后来发现了 default lang 不再是 groovy 而变成了 painless ，而 painless 的取值需要携带 key ，即 params.tag 这样才可以正常找到值。那么 groovy 为什么不再是 default 还被新版本中标记为 deprecated 了呢……要知道 groovy 之前替换掉 mvel 的理由是足够快而且简单……</p><p>先说一下什么是 sandboxed language </p><blockquote><p>沙盒是在受限的安全环境中运行应用程序的一种做法，这种做法是要限制授予应用程序的代码访问权限。</p></blockquote><p>像 groovy 和 JavaScript 这类脚本语言它们本身都不是 sandboxed，它们可以做很多系统级别的不止是读写、网络请求的操作，这样就给基于 JAVA 并且在运行中默认开启 <a href="https://docs.oracle.com/javase/tutorial/essential/environment/security.html" target="_blank" rel="noopener">The Security Manager</a> 的 elasticsearch 带来很大的安全隐患，比如在脚本中随便加一句 infinite loop ，服务器可能就表现成拒绝访问的状态，所以在之前的版本中 elasticsearch 为 groovy 加入了沙盒控制一些权限，然而后面由于权限限制不够，还是出现了一些问题23333。</p><p>虽然 5.x 仍然内置 groovy ，但是考虑到 elasticsearch script 的来源，可以是 inline、store 、还有 file，前两个就不说了，一个是 query 中直接写进去的像我上面的例子，另一个也是以数据的形式存在某个 cluster state 的 _script 节点下，而 file 的形式是配置在 elasticsearch 的 config 文件夹中，所以从安全的角度，elasticsearch 5.x 只对 file script 默认允许执行 groovy 。</p><p>至于开头的 query ，如果不考虑安全性，例如默认我们的 elasticsearch 运行与一个相对隔离的环境下，如果还想用 inline groovy ，就可以为 groovy 单独开启一个配置 <code>script.engine.groovy.inline: true</code> 或者更宽泛的，针对所有 inline script 的配置<code>script.inline: true</code>，那么为上面的 script 声明一下 lang 就可以成功执行了（在 Python 和 Node.js 包中拼 dict 和 object 也是一样）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST index/type/id/_update</span><br><span class="line">&#123;</span><br><span class="line">    &quot;script&quot; :&#123;</span><br><span class="line">      &quot;inline&quot;: &quot;ctx._source.tags -= tag&quot;,</span><br><span class="line">      &quot;lang&quot;: &quot;groovy&quot;,</span><br><span class="line">      &quot;params&quot; : &#123;</span><br><span class="line">          &quot;tag&quot; : &quot;blue&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>等 painless 相对稳定了，直接切换过去就可以了，毕竟语法都类似，而且还安全。</p><h4 id="upsert-element-to-exist-document"><a href="#upsert-element-to-exist-document" class="headerlink" title="upsert element to exist document"></a>upsert element to exist document</h4><p>一个已经存在的 document 可能有一个 tags 的 element ，它是一个 Array 形态，现在我们想 upsert 某个 tag 进去。</p><p>这种 array 的操作通常是用 script 操作的，于是很直观地用到文档中的 upsert:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;inline&quot;: &quot;ctx._source.tags += tag&quot;,</span><br><span class="line">    &quot;lang&quot;: &quot;groovy&quot;,</span><br><span class="line">    &quot;params&quot;: &#123;</span><br><span class="line">      &quot;tag&quot;: &quot;皮皮虾&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;upsert&quot;: &#123;</span><br><span class="line">    &quot;tags&quot;: [&quot;皮皮虾&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然而 script 总是执行而 upsert 不执行，原因是 document 已经存在了，这个 upsert 只是针对当 document 不存在时，所以还是要把逻辑做在 script 中:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;inline&quot;: &quot;if (ctx._source.tags) &#123;ctx._source.tags += tag;&#125; else &#123;ctx._source.tags = [tag]&#125;&quot;,</span><br><span class="line">    &quot;lang&quot;: &quot;groovy&quot;,</span><br><span class="line">    &quot;params&quot;: &#123;</span><br><span class="line">      &quot;tag&quot;: &quot;皮皮虾&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="重启集群节点"><a href="#重启集群节点" class="headerlink" title="重启集群节点"></a>重启集群节点</h2><p>（以下内容出现的部分流程和命令可以在 《ELASTICSEARCH 源码解析与优化实战》中找到详细的解释，部分内容依据我们的业务场景、部署方式、机器规格有额外的修改）</p><p>由于机器升级或某个节点出现了问题，需要执行重启的操作。但是直接重启会引发新插入更新的文档和重启节点的版本不匹配触发全局的 allocation 导致部分 indices 在 recovery 过程中长时间不可用。所以比较平滑的重启方法（rolling restart）如下：</p><ol><li><p>检查集群的状态，不太建议在红色状态下重启，至少是黄绿状态最好 primary shards 都是可用的状态</p></li><li><p>停止切片分配</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -XPUT localhost:9200/_cluster/settings -d '&#123;"transient":&#123;"cluster.routing.allocation.enable": "none"&#125;&#125;'</span><br></pre></td></tr></table></figure></li><li><p>执行 synced flush，解决副本分片恢复慢的问题</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -X POST "localhost:9200/_flush/synced"</span><br></pre></td></tr></table></figure></li><li><p>重启节点</p></li><li><p>调整分片的限速，这一步考虑集群的配置，主要是内存和带宽</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -X PUT "localhost:9200/_cluster/settings?flat_settings=true" -H 'Content-Type: application/json' -d'&#123;"transient" : &#123;"indices.recovery.max_bytes_per_sec" : "300mb"&#125;&#125;'</span><br><span class="line">curl -X PUT "localhost:9200/_cluster/settings?flat_settings=true" -H 'Content-Type: application/json' -d'&#123;"transient" : &#123;"cluster.routing.allocation.node_concurrent_recoveries" : "100"&#125;&#125;'</span><br></pre></td></tr></table></figure></li><li><p>开启切片分配</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -XPUT localhost:9200/_cluster/settings -d '&#123;"transient":&#123;"cluster.routing.allocation.enable": "all"&#125;&#125;'</span><br></pre></td></tr></table></figure></li></ol><p>等待切片分配完成，期间观察集群的可用状态。</p><h2 id="集群水平扩容"><a href="#集群水平扩容" class="headerlink" title="集群水平扩容"></a>集群水平扩容</h2><p>扩容比较简单，如果集群是上云的，可以直接用加入新节点的方式平衡 data 的储存分配。</p><p>只是加入的时候要注意 除了 elasticsearch.yml 的配置要争取之外，jvm.options 的 heap size 也一定要符合机器的水平并且尽量与集群中其他同角色节点的配置<strong>完全相同</strong>，否则会出现 heap size 过小频繁触发 full GC 导致节点故障甚至切片分配问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;or-operation&quot;&gt;&lt;a href=&quot;#or-operation&quot; class=&quot;headerlink&quot; title=&quot;or operation&quot;&gt;&lt;/a&gt;or operation&lt;/h2&gt;&lt;p&gt;以下内容基于 Elasticsearch 2.0.1 。&lt;/
      
    
    </summary>
    
    
  </entry>
  
</feed>
